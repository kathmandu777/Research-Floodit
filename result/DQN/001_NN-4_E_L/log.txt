
(RL) C:\Users\kator\OneDrive\ドキュメント\ResearchFloodit\gym-floodit>python C:\Users\kator\OneDrive\ドキュメント\ResearchFloodit\gym-floodit\floodit-DQN.py
Using TensorFlow backend.
D:\Anaconda\envs\RL\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
D:\Anaconda\envs\RL\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
D:\Anaconda\envs\RL\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
D:\Anaconda\envs\RL\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
D:\Anaconda\envs\RL\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
D:\Anaconda\envs\RL\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
D:\Anaconda\envs\RL\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
D:\Anaconda\envs\RL\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
D:\Anaconda\envs\RL\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
D:\Anaconda\envs\RL\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
D:\Anaconda\envs\RL\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
D:\Anaconda\envs\RL\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
pygame 2.0.1 (SDL 2.0.14, Python 3.7.9)
Hello from the pygame community. https://www.pygame.org/contribute.html
libpng warning: iCCP: known incorrect sRGB profile
libpng warning: iCCP: known incorrect sRGB profile
libpng warning: iCCP: known incorrect sRGB profile
libpng warning: iCCP: known incorrect sRGB profile
libpng warning: iCCP: known incorrect sRGB profile

**************************************************  Model  **************************************************

Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten_1 (Flatten)          (None, 36)                0
_________________________________________________________________
dense_1 (Dense)              (None, 32)                1184
_________________________________________________________________
activation_1 (Activation)    (None, 32)                0
_________________________________________________________________
dense_2 (Dense)              (None, 32)                1056
_________________________________________________________________
activation_2 (Activation)    (None, 32)                0
_________________________________________________________________
dense_3 (Dense)              (None, 32)                1056
_________________________________________________________________
activation_3 (Activation)    (None, 32)                0
_________________________________________________________________
dense_4 (Dense)              (None, 6)                 198
_________________________________________________________________
activation_4 (Activation)    (None, 6)                 0
=================================================================
Total params: 3,494
Trainable params: 3,494
Non-trainable params: 0
_________________________________________________________________
None
2021-02-24 14:08:07.743066: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2

**************************************************  History  **************************************************
Training for 1000000 steps ...
Interval 1 (0 steps performed)
WARNING:tensorflow:From D:\Anaconda\envs\RL\lib\site-packages\keras\backend\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

   98/10000 [..............................] - ETA: 1:01 - reward: -0.36052021-02-24 14:08:08.984889: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2021-02-24 14:08:08.992432: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
2021-02-24 14:08:09.016189: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] remapper failed: Invalid argument: The graph couldn't be sorted in topological order.
2021-02-24 14:08:09.022240: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] arithmetic_optimizer failed: Invalid argument: The graph couldn't be sorted in topological order.
2021-02-24 14:08:09.031398: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 0, topological sort failed with message: The graph couldn't be sorted in topological order.
2021-02-24 14:08:09.040046: E tensorflow/core/grappler/optimizers/dependency_optimizer.cc:697] Iteration = 1, topological sort failed with message: The graph couldn't be sorted in topological order.
10000/10000 [==============================] - 108s 11ms/step - reward: -0.1302
5871 episodes - episode_reward: -0.222 [-0.500, 1.944] - loss: 0.035 - mae: 0.417 - mean_q: -0.151 - isWon: 0.006 - isLose: 0.000 - changed_square: 5.571 - life: 8.373 - isQuit: 0.000

Interval 2 (10000 steps performed)
10000/10000 [==============================] - 97s 10ms/step - reward: 0.0044
4391 episodes - episode_reward: 0.010 [-0.500, 1.944] - loss: 0.038 - mae: 0.358 - mean_q: -0.015 - isWon: 0.024 - isLose: 0.000 - changed_square: 6.821 - life: 7.998 - isQuit: 0.000

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 94s 9ms/step - reward: 0.0401
4043 episodes - episode_reward: 0.099 [-0.500, 1.944] - loss: 0.045 - mae: 0.345 - mean_q: 0.072 - isWon: 0.031 - isLose: 0.000 - changed_square: 7.099 - life: 7.907 - isQuit: 0.000

Interval 4 (30000 steps performed)
10000/10000 [==============================] - 96s 10ms/step - reward: 0.0616
3777 episodes - episode_reward: 0.163 [-0.500, 1.944] - loss: 0.052 - mae: 0.336 - mean_q: 0.135 - isWon: 0.036 - isLose: 0.000 - changed_square: 7.117 - life: 7.816 - isQuit: 0.000

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 94s 9ms/step - reward: 0.0624
3731 episodes - episode_reward: 0.167 [-0.500, 1.944] - loss: 0.060 - mae: 0.329 - mean_q: 0.178 - isWon: 0.036 - isLose: 0.000 - changed_square: 7.098 - life: 7.809 - isQuit: 0.000

Interval 6 (50000 steps performed)
10000/10000 [==============================] - 94s 9ms/step - reward: 0.0918
3535 episodes - episode_reward: 0.260 [-0.500, 1.944] - loss: 0.063 - mae: 0.323 - mean_q: 0.216 - isWon: 0.046 - isLose: 0.000 - changed_square: 7.271 - life: 7.756 - isQuit: 0.000

Interval 7 (60000 steps performed)
10000/10000 [==============================] - 94s 9ms/step - reward: 0.0917
3556 episodes - episode_reward: 0.258 [-0.500, 1.944] - loss: 0.069 - mae: 0.316 - mean_q: 0.261 - isWon: 0.047 - isLose: 0.000 - changed_square: 7.231 - life: 7.747 - isQuit: 0.000

Interval 8 (70000 steps performed)
10000/10000 [==============================] - 93s 9ms/step - reward: 0.1021
3539 episodes - episode_reward: 0.288 [-0.500, 1.944] - loss: 0.071 - mae: 0.314 - mean_q: 0.282 - isWon: 0.051 - isLose: 0.000 - changed_square: 7.362 - life: 7.743 - isQuit: 0.000

Interval 9 (80000 steps performed)
10000/10000 [==============================] - 93s 9ms/step - reward: 0.1058
3450 episodes - episode_reward: 0.307 [-0.500, 1.944] - loss: 0.075 - mae: 0.317 - mean_q: 0.319 - isWon: 0.052 - isLose: 0.000 - changed_square: 7.308 - life: 7.707 - isQuit: 0.000

Interval 10 (90000 steps performed)
10000/10000 [==============================] - 90s 9ms/step - reward: 0.1221
3401 episodes - episode_reward: 0.359 [-0.500, 1.944] - loss: 0.077 - mae: 0.318 - mean_q: 0.354 - isWon: 0.060 - isLose: 0.000 - changed_square: 7.359 - life: 7.693 - isQuit: 0.000

Interval 11 (100000 steps performed)
10000/10000 [==============================] - 93s 9ms/step - reward: 0.1308
3386 episodes - episode_reward: 0.386 [-0.500, 1.944] - loss: 0.080 - mae: 0.318 - mean_q: 0.399 - isWon: 0.066 - isLose: 0.000 - changed_square: 7.348 - life: 7.686 - isQuit: 0.000

Interval 12 (110000 steps performed)
10000/10000 [==============================] - 97s 10ms/step - reward: 0.1398
3282 episodes - episode_reward: 0.426 [-0.500, 1.944] - loss: 0.083 - mae: 0.316 - mean_q: 0.436 - isWon: 0.068 - isLose: 0.000 - changed_square: 7.361 - life: 7.647 - isQuit: 0.000

Interval 13 (120000 steps performed)
10000/10000 [==============================] - 93s 9ms/step - reward: 0.1395
3314 episodes - episode_reward: 0.421 [-0.500, 1.944] - loss: 0.085 - mae: 0.316 - mean_q: 0.464 - isWon: 0.070 - isLose: 0.000 - changed_square: 7.324 - life: 7.652 - isQuit: 0.000

Interval 14 (130000 steps performed)
10000/10000 [==============================] - 94s 9ms/step - reward: 0.1468
3279 episodes - episode_reward: 0.448 [-0.500, 1.944] - loss: 0.088 - mae: 0.319 - mean_q: 0.503 - isWon: 0.074 - isLose: 0.000 - changed_square: 7.302 - life: 7.637 - isQuit: 0.000

Interval 15 (140000 steps performed)
10000/10000 [==============================] - 92s 9ms/step - reward: 0.1572
3269 episodes - episode_reward: 0.481 [-0.500, 1.944] - loss: 0.089 - mae: 0.325 - mean_q: 0.535 - isWon: 0.081 - isLose: 0.000 - changed_square: 7.300 - life: 7.640 - isQuit: 0.000

Interval 16 (150000 steps performed)
10000/10000 [==============================] - 91s 9ms/step - reward: 0.1676
3235 episodes - episode_reward: 0.518 [-0.500, 1.944] - loss: 0.091 - mae: 0.329 - mean_q: 0.570 - isWon: 0.086 - isLose: 0.000 - changed_square: 7.373 - life: 7.625 - isQuit: 0.000

Interval 17 (160000 steps performed)
10000/10000 [==============================] - 92s 9ms/step - reward: 0.1684
3191 episodes - episode_reward: 0.527 [-0.500, 1.944] - loss: 0.093 - mae: 0.333 - mean_q: 0.594 - isWon: 0.086 - isLose: 0.000 - changed_square: 7.277 - life: 7.597 - isQuit: 0.000

Interval 18 (170000 steps performed)
10000/10000 [==============================] - 91s 9ms/step - reward: 0.1744
3191 episodes - episode_reward: 0.547 [-0.500, 1.944] - loss: 0.093 - mae: 0.332 - mean_q: 0.611 - isWon: 0.090 - isLose: 0.000 - changed_square: 7.287 - life: 7.585 - isQuit: 0.000

Interval 19 (180000 steps performed)
10000/10000 [==============================] - 91s 9ms/step - reward: 0.1796
3173 episodes - episode_reward: 0.566 [-0.500, 1.944] - loss: 0.095 - mae: 0.335 - mean_q: 0.646 - isWon: 0.093 - isLose: 0.000 - changed_square: 7.308 - life: 7.579 - isQuit: 0.000

Interval 20 (190000 steps performed)
10000/10000 [==============================] - 91s 9ms/step - reward: 0.1838
3166 episodes - episode_reward: 0.581 [-0.500, 1.944] - loss: 0.095 - mae: 0.337 - mean_q: 0.667 - isWon: 0.096 - isLose: 0.000 - changed_square: 7.297 - life: 7.575 - isQuit: 0.000

Interval 21 (200000 steps performed)
10000/10000 [==============================] - 91s 9ms/step - reward: 0.1912
3109 episodes - episode_reward: 0.615 [-0.500, 1.944] - loss: 0.097 - mae: 0.339 - mean_q: 0.699 - isWon: 0.100 - isLose: 0.000 - changed_square: 7.229 - life: 7.556 - isQuit: 0.000

Interval 22 (210000 steps performed)
10000/10000 [==============================] - 91s 9ms/step - reward: 0.1994
3094 episodes - episode_reward: 0.644 [-0.500, 1.944] - loss: 0.098 - mae: 0.338 - mean_q: 0.738 - isWon: 0.106 - isLose: 0.000 - changed_square: 7.176 - life: 7.539 - isQuit: 0.000

Interval 23 (220000 steps performed)
10000/10000 [==============================] - 90s 9ms/step - reward: 0.1993
3048 episodes - episode_reward: 0.654 [-0.500, 1.944] - loss: 0.097 - mae: 0.345 - mean_q: 0.757 - isWon: 0.103 - isLose: 0.000 - changed_square: 7.238 - life: 7.524 - isQuit: 0.000

Interval 24 (230000 steps performed)
10000/10000 [==============================] - 91s 9ms/step - reward: 0.2000
3078 episodes - episode_reward: 0.649 [-0.500, 1.944] - loss: 0.097 - mae: 0.343 - mean_q: 0.784 - isWon: 0.104 - isLose: 0.000 - changed_square: 7.285 - life: 7.539 - isQuit: 0.000

Interval 25 (240000 steps performed)
10000/10000 [==============================] - 90s 9ms/step - reward: 0.2042
3032 episodes - episode_reward: 0.674 [-0.500, 1.944] - loss: 0.096 - mae: 0.344 - mean_q: 0.802 - isWon: 0.108 - isLose: 0.000 - changed_square: 7.140 - life: 7.517 - isQuit: 0.000

Interval 26 (250000 steps performed)
10000/10000 [==============================] - 90s 9ms/step - reward: 0.2150
3032 episodes - episode_reward: 0.709 [-0.500, 1.944] - loss: 0.095 - mae: 0.342 - mean_q: 0.806 - isWon: 0.112 - isLose: 0.000 - changed_square: 7.318 - life: 7.528 - isQuit: 0.000

Interval 27 (260000 steps performed)
10000/10000 [==============================] - 91s 9ms/step - reward: 0.2132
3034 episodes - episode_reward: 0.703 [-0.500, 1.944] - loss: 0.094 - mae: 0.343 - mean_q: 0.819 - isWon: 0.111 - isLose: 0.000 - changed_square: 7.303 - life: 7.517 - isQuit: 0.000

Interval 28 (270000 steps performed)
10000/10000 [==============================] - 90s 9ms/step - reward: 0.2174
2944 episodes - episode_reward: 0.739 [-0.500, 1.944] - loss: 0.094 - mae: 0.344 - mean_q: 0.835 - isWon: 0.113 - isLose: 0.000 - changed_square: 7.196 - life: 7.479 - isQuit: 0.000

Interval 29 (280000 steps performed)
10000/10000 [==============================] - 90s 9ms/step - reward: 0.2300
2981 episodes - episode_reward: 0.772 [-0.500, 1.944] - loss: 0.093 - mae: 0.345 - mean_q: 0.859 - isWon: 0.122 - isLose: 0.000 - changed_square: 7.223 - life: 7.488 - isQuit: 0.000

Interval 30 (290000 steps performed)
10000/10000 [==============================] - 93s 9ms/step - reward: 0.2282
2959 episodes - episode_reward: 0.771 [-0.500, 1.944] - loss: 0.093 - mae: 0.348 - mean_q: 0.882 - isWon: 0.120 - isLose: 0.000 - changed_square: 7.251 - life: 7.470 - isQuit: 0.000

Interval 31 (300000 steps performed)
10000/10000 [==============================] - 91s 9ms/step - reward: 0.2308
2937 episodes - episode_reward: 0.786 [-0.500, 1.944] - loss: 0.093 - mae: 0.354 - mean_q: 0.896 - isWon: 0.121 - isLose: 0.000 - changed_square: 7.250 - life: 7.464 - isQuit: 0.000

Interval 32 (310000 steps performed)
10000/10000 [==============================] - 87s 9ms/step - reward: 0.2270
2932 episodes - episode_reward: 0.774 [-0.500, 1.944] - loss: 0.092 - mae: 0.353 - mean_q: 0.911 - isWon: 0.119 - isLose: 0.000 - changed_square: 7.217 - life: 7.462 - isQuit: 0.000

Interval 33 (320000 steps performed)
10000/10000 [==============================] - 86s 9ms/step - reward: 0.2298
2904 episodes - episode_reward: 0.791 [-0.500, 1.944] - loss: 0.091 - mae: 0.350 - mean_q: 0.908 - isWon: 0.120 - isLose: 0.000 - changed_square: 7.196 - life: 7.449 - isQuit: 0.000

Interval 34 (330000 steps performed)
10000/10000 [==============================] - 86s 9ms/step - reward: 0.2309
2915 episodes - episode_reward: 0.792 [-0.500, 1.944] - loss: 0.091 - mae: 0.348 - mean_q: 0.918 - isWon: 0.122 - isLose: 0.000 - changed_square: 7.130 - life: 7.450 - isQuit: 0.000

Interval 35 (340000 steps performed)
10000/10000 [==============================] - 86s 9ms/step - reward: 0.2367
2961 episodes - episode_reward: 0.799 [-0.500, 1.944] - loss: 0.092 - mae: 0.351 - mean_q: 0.930 - isWon: 0.128 - isLose: 0.000 - changed_square: 7.131 - life: 7.480 - isQuit: 0.000

Interval 36 (350000 steps performed)
10000/10000 [==============================] - 85s 9ms/step - reward: 0.2328
2926 episodes - episode_reward: 0.795 [-0.500, 1.944] - loss: 0.092 - mae: 0.349 - mean_q: 0.938 - isWon: 0.123 - isLose: 0.000 - changed_square: 7.179 - life: 7.466 - isQuit: 0.000

Interval 37 (360000 steps performed)
10000/10000 [==============================] - 85s 9ms/step - reward: 0.2306
2929 episodes - episode_reward: 0.788 [-0.500, 1.944] - loss: 0.090 - mae: 0.351 - mean_q: 0.940 - isWon: 0.121 - isLose: 0.000 - changed_square: 7.211 - life: 7.463 - isQuit: 0.000

Interval 38 (370000 steps performed)
10000/10000 [==============================] - 88s 9ms/step - reward: 0.2358
2909 episodes - episode_reward: 0.811 [-0.500, 1.944] - loss: 0.091 - mae: 0.355 - mean_q: 0.944 - isWon: 0.124 - isLose: 0.000 - changed_square: 7.189 - life: 7.445 - isQuit: 0.000

Interval 39 (380000 steps performed)
10000/10000 [==============================] - 89s 9ms/step - reward: 0.2306
2884 episodes - episode_reward: 0.799 [-0.500, 1.944] - loss: 0.091 - mae: 0.350 - mean_q: 0.936 - isWon: 0.120 - isLose: 0.000 - changed_square: 7.164 - life: 7.437 - isQuit: 0.000

Interval 40 (390000 steps performed)
10000/10000 [==============================] - 92s 9ms/step - reward: 0.2369
2887 episodes - episode_reward: 0.820 [-0.500, 1.944] - loss: 0.090 - mae: 0.353 - mean_q: 0.948 - isWon: 0.127 - isLose: 0.000 - changed_square: 7.075 - life: 7.425 - isQuit: 0.000

Interval 41 (400000 steps performed)
10000/10000 [==============================] - 95s 10ms/step - reward: 0.2432
2854 episodes - episode_reward: 0.852 [-0.500, 1.944] - loss: 0.089 - mae: 0.354 - mean_q: 0.962 - isWon: 0.128 - isLose: 0.000 - changed_square: 7.150 - life: 7.420 - isQuit: 0.000

Interval 42 (410000 steps performed)
10000/10000 [==============================] - 91s 9ms/step - reward: 0.2391
2884 episodes - episode_reward: 0.829 [-0.500, 1.944] - loss: 0.089 - mae: 0.350 - mean_q: 0.962 - isWon: 0.125 - isLose: 0.000 - changed_square: 7.240 - life: 7.442 - isQuit: 0.000

Interval 43 (420000 steps performed)
10000/10000 [==============================] - 89s 9ms/step - reward: 0.2531
2865 episodes - episode_reward: 0.884 [-0.500, 1.944] - loss: 0.088 - mae: 0.349 - mean_q: 0.971 - isWon: 0.135 - isLose: 0.000 - changed_square: 7.166 - life: 7.441 - isQuit: 0.000

Interval 44 (430000 steps performed)
10000/10000 [==============================] - 88s 9ms/step - reward: 0.2393
2894 episodes - episode_reward: 0.827 [-0.500, 1.944] - loss: 0.087 - mae: 0.347 - mean_q: 0.986 - isWon: 0.128 - isLose: 0.000 - changed_square: 7.081 - life: 7.445 - isQuit: 0.000

Interval 45 (440000 steps performed)
10000/10000 [==============================] - 85s 8ms/step - reward: 0.2422
2851 episodes - episode_reward: 0.849 [-0.500, 1.944] - loss: 0.087 - mae: 0.343 - mean_q: 0.977 - isWon: 0.128 - isLose: 0.000 - changed_square: 7.138 - life: 7.406 - isQuit: 0.000

Interval 46 (450000 steps performed)
10000/10000 [==============================] - 89s 9ms/step - reward: 0.2469
2877 episodes - episode_reward: 0.858 [-0.500, 1.944] - loss: 0.088 - mae: 0.342 - mean_q: 0.992 - isWon: 0.132 - isLose: 0.000 - changed_square: 7.109 - life: 7.424 - isQuit: 0.000

Interval 47 (460000 steps performed)
10000/10000 [==============================] - 93s 9ms/step - reward: 0.2456
2866 episodes - episode_reward: 0.857 [-0.500, 1.944] - loss: 0.088 - mae: 0.344 - mean_q: 0.998 - isWon: 0.132 - isLose: 0.000 - changed_square: 7.082 - life: 7.427 - isQuit: 0.000

Interval 48 (470000 steps performed)
10000/10000 [==============================] - 91s 9ms/step - reward: 0.2521
2888 episodes - episode_reward: 0.873 [-0.500, 1.944] - loss: 0.089 - mae: 0.345 - mean_q: 0.998 - isWon: 0.135 - isLose: 0.000 - changed_square: 7.168 - life: 7.448 - isQuit: 0.000

Interval 49 (480000 steps performed)
10000/10000 [==============================] - 85s 9ms/step - reward: 0.2438
2835 episodes - episode_reward: 0.860 [-0.500, 1.944] - loss: 0.088 - mae: 0.348 - mean_q: 0.997 - isWon: 0.131 - isLose: 0.000 - changed_square: 7.027 - life: 7.397 - isQuit: 0.000

Interval 50 (490000 steps performed)
10000/10000 [==============================] - 87s 9ms/step - reward: 0.2511
2858 episodes - episode_reward: 0.879 [-0.500, 1.944] - loss: 0.088 - mae: 0.350 - mean_q: 1.005 - isWon: 0.132 - isLose: 0.000 - changed_square: 7.232 - life: 7.434 - isQuit: 0.000

Interval 51 (500000 steps performed)
10000/10000 [==============================] - 84s 8ms/step - reward: 0.2518
2856 episodes - episode_reward: 0.881 [-0.500, 1.944] - loss: 0.087 - mae: 0.358 - mean_q: 1.014 - isWon: 0.134 - isLose: 0.000 - changed_square: 7.145 - life: 7.418 - isQuit: 0.000

Interval 52 (510000 steps performed)
10000/10000 [==============================] - 85s 8ms/step - reward: 0.2532
2860 episodes - episode_reward: 0.886 [-0.500, 1.944] - loss: 0.087 - mae: 0.361 - mean_q: 1.019 - isWon: 0.136 - isLose: 0.000 - changed_square: 7.118 - life: 7.421 - isQuit: 0.000

Interval 53 (520000 steps performed)
10000/10000 [==============================] - 89s 9ms/step - reward: 0.2553
2829 episodes - episode_reward: 0.902 [-0.500, 1.944] - loss: 0.087 - mae: 0.364 - mean_q: 1.024 - isWon: 0.136 - isLose: 0.000 - changed_square: 7.152 - life: 7.405 - isQuit: 0.000

Interval 54 (530000 steps performed)
10000/10000 [==============================] - 85s 8ms/step - reward: 0.2595
2863 episodes - episode_reward: 0.906 [-0.500, 1.944] - loss: 0.086 - mae: 0.367 - mean_q: 1.031 - isWon: 0.141 - isLose: 0.000 - changed_square: 7.111 - life: 7.424 - isQuit: 0.000

Interval 55 (540000 steps performed)
10000/10000 [==============================] - 89s 9ms/step - reward: 0.2569
2849 episodes - episode_reward: 0.902 [-0.500, 1.944] - loss: 0.086 - mae: 0.364 - mean_q: 1.037 - isWon: 0.139 - isLose: 0.000 - changed_square: 7.075 - life: 7.415 - isQuit: 0.000

Interval 56 (550000 steps performed)
10000/10000 [==============================] - 84s 8ms/step - reward: 0.2528
2822 episodes - episode_reward: 0.896 [-0.500, 1.944] - loss: 0.087 - mae: 0.364 - mean_q: 1.040 - isWon: 0.134 - isLose: 0.000 - changed_square: 7.130 - life: 7.417 - isQuit: 0.000

Interval 57 (560000 steps performed)
10000/10000 [==============================] - 85s 8ms/step - reward: 0.2601
2814 episodes - episode_reward: 0.924 [-0.500, 1.944] - loss: 0.087 - mae: 0.364 - mean_q: 1.047 - isWon: 0.139 - isLose: 0.000 - changed_square: 7.115 - life: 7.390 - isQuit: 0.000

Interval 58 (570000 steps performed)
10000/10000 [==============================] - 85s 8ms/step - reward: 0.2553
2809 episodes - episode_reward: 0.909 [-0.500, 1.944] - loss: 0.086 - mae: 0.368 - mean_q: 1.052 - isWon: 0.136 - isLose: 0.000 - changed_square: 7.083 - life: 7.395 - isQuit: 0.000

Interval 59 (580000 steps performed)
10000/10000 [==============================] - 86s 9ms/step - reward: 0.2643
2833 episodes - episode_reward: 0.933 [-0.500, 1.944] - loss: 0.084 - mae: 0.370 - mean_q: 1.061 - isWon: 0.142 - isLose: 0.000 - changed_square: 7.155 - life: 7.413 - isQuit: 0.000

Interval 60 (590000 steps performed)
10000/10000 [==============================] - 84s 8ms/step - reward: 0.2538
2822 episodes - episode_reward: 0.899 [-0.500, 1.944] - loss: 0.083 - mae: 0.367 - mean_q: 1.063 - isWon: 0.136 - isLose: 0.000 - changed_square: 7.065 - life: 7.399 - isQuit: 0.000

Interval 61 (600000 steps performed)
10000/10000 [==============================] - 87s 9ms/step - reward: 0.2669
2847 episodes - episode_reward: 0.938 [-0.500, 1.944] - loss: 0.083 - mae: 0.366 - mean_q: 1.065 - isWon: 0.145 - isLose: 0.000 - changed_square: 7.134 - life: 7.422 - isQuit: 0.000

Interval 62 (610000 steps performed)
10000/10000 [==============================] - 86s 9ms/step - reward: 0.2600
2845 episodes - episode_reward: 0.913 [-0.500, 1.944] - loss: 0.083 - mae: 0.368 - mean_q: 1.062 - isWon: 0.141 - isLose: 0.000 - changed_square: 7.072 - life: 7.423 - isQuit: 0.000

Interval 63 (620000 steps performed)
10000/10000 [==============================] - 84s 8ms/step - reward: 0.2645
2785 episodes - episode_reward: 0.950 [-0.500, 1.944] - loss: 0.083 - mae: 0.365 - mean_q: 1.061 - isWon: 0.141 - isLose: 0.000 - changed_square: 7.134 - life: 7.393 - isQuit: 0.000

Interval 64 (630000 steps performed)
10000/10000 [==============================] - 130s 13ms/step - reward: 0.2599
2814 episodes - episode_reward: 0.923 [-0.500, 1.944] - loss: 0.083 - mae: 0.361 - mean_q: 1.065 - isWon: 0.140 - isLose: 0.000 - changed_square: 7.044 - life: 7.392 - isQuit: 0.000

Interval 65 (640000 steps performed)
10000/10000 [==============================] - 84s 8ms/step - reward: 0.2620
2798 episodes - episode_reward: 0.937 [-0.500, 1.944] - loss: 0.083 - mae: 0.364 - mean_q: 1.068 - isWon: 0.141 - isLose: 0.000 - changed_square: 7.048 - life: 7.396 - isQuit: 0.000

Interval 66 (650000 steps performed)
10000/10000 [==============================] - 85s 8ms/step - reward: 0.2567
2826 episodes - episode_reward: 0.908 [-0.500, 1.944] - loss: 0.082 - mae: 0.367 - mean_q: 1.080 - isWon: 0.138 - isLose: 0.000 - changed_square: 7.074 - life: 7.407 - isQuit: 0.000

Interval 67 (660000 steps performed)
10000/10000 [==============================] - 85s 8ms/step - reward: 0.2619
2826 episodes - episode_reward: 0.927 [-0.500, 1.944] - loss: 0.082 - mae: 0.370 - mean_q: 1.073 - isWon: 0.142 - isLose: 0.000 - changed_square: 7.063 - life: 7.406 - isQuit: 0.000

Interval 68 (670000 steps performed)
10000/10000 [==============================] - 85s 9ms/step - reward: 0.2601
2814 episodes - episode_reward: 0.924 [-0.500, 1.944] - loss: 0.081 - mae: 0.373 - mean_q: 1.074 - isWon: 0.139 - isLose: 0.000 - changed_square: 7.132 - life: 7.408 - isQuit: 0.000

Interval 69 (680000 steps performed)
10000/10000 [==============================] - 86s 9ms/step - reward: 0.2639
2801 episodes - episode_reward: 0.942 [-0.500, 1.944] - loss: 0.081 - mae: 0.368 - mean_q: 1.075 - isWon: 0.141 - isLose: 0.000 - changed_square: 7.131 - life: 7.405 - isQuit: 0.000

Interval 70 (690000 steps performed)
10000/10000 [==============================] - 86s 9ms/step - reward: 0.2623
2835 episodes - episode_reward: 0.925 [-0.500, 1.944] - loss: 0.081 - mae: 0.367 - mean_q: 1.075 - isWon: 0.142 - isLose: 0.000 - changed_square: 7.090 - life: 7.404 - isQuit: 0.000

Interval 71 (700000 steps performed)
10000/10000 [==============================] - 88s 9ms/step - reward: 0.2658
2819 episodes - episode_reward: 0.943 [-0.500, 1.944] - loss: 0.082 - mae: 0.366 - mean_q: 1.072 - isWon: 0.143 - isLose: 0.000 - changed_square: 7.137 - life: 7.399 - isQuit: 0.000

Interval 72 (710000 steps performed)
10000/10000 [==============================] - 89s 9ms/step - reward: 0.2630
2828 episodes - episode_reward: 0.930 [-0.500, 1.944] - loss: 0.082 - mae: 0.362 - mean_q: 1.075 - isWon: 0.141 - isLose: 0.000 - changed_square: 7.142 - life: 7.412 - isQuit: 0.000

Interval 73 (720000 steps performed)
10000/10000 [==============================] - 96s 10ms/step - reward: 0.2650
2762 episodes - episode_reward: 0.959 [-0.500, 1.944] - loss: 0.082 - mae: 0.364 - mean_q: 1.075 - isWon: 0.143 - isLose: 0.000 - changed_square: 7.003 - life: 7.380 - isQuit: 0.000

Interval 74 (730000 steps performed)
10000/10000 [==============================] - 94s 9ms/step - reward: 0.2610
2824 episodes - episode_reward: 0.924 [-0.500, 1.944] - loss: 0.081 - mae: 0.364 - mean_q: 1.078 - isWon: 0.142 - isLose: 0.000 - changed_square: 7.025 - life: 7.393 - isQuit: 0.000

Interval 75 (740000 steps performed)
10000/10000 [==============================] - 90s 9ms/step - reward: 0.2621
2799 episodes - episode_reward: 0.936 [-0.500, 1.944] - loss: 0.082 - mae: 0.365 - mean_q: 1.080 - isWon: 0.141 - isLose: 0.000 - changed_square: 7.031 - life: 7.389 - isQuit: 0.000

Interval 76 (750000 steps performed)
10000/10000 [==============================] - 120s 12ms/step - reward: 0.2648
2812 episodes - episode_reward: 0.942 [-0.500, 1.944] - loss: 0.081 - mae: 0.364 - mean_q: 1.080 - isWon: 0.142 - isLose: 0.000 - changed_square: 7.118 - life: 7.405 - isQuit: 0.000

Interval 77 (760000 steps performed)
10000/10000 [==============================] - 96s 10ms/step - reward: 0.2646
2800 episodes - episode_reward: 0.945 [-0.500, 1.944] - loss: 0.081 - mae: 0.364 - mean_q: 1.085 - isWon: 0.143 - isLose: 0.000 - changed_square: 7.064 - life: 7.381 - isQuit: 0.000

Interval 78 (770000 steps performed)
10000/10000 [==============================] - 85s 9ms/step - reward: 0.2671
2751 episodes - episode_reward: 0.970 [-0.500, 1.944] - loss: 0.081 - mae: 0.361 - mean_q: 1.086 - isWon: 0.143 - isLose: 0.000 - changed_square: 7.039 - life: 7.371 - isQuit: 0.000

Interval 79 (780000 steps performed)
10000/10000 [==============================] - 85s 8ms/step - reward: 0.2703
2796 episodes - episode_reward: 0.967 [-0.500, 1.944] - loss: 0.081 - mae: 0.359 - mean_q: 1.083 - isWon: 0.146 - isLose: 0.000 - changed_square: 7.098 - life: 7.396 - isQuit: 0.000

Interval 80 (790000 steps performed)
10000/10000 [==============================] - 84s 8ms/step - reward: 0.2593
2830 episodes - episode_reward: 0.916 [-0.500, 1.944] - loss: 0.081 - mae: 0.362 - mean_q: 1.082 - isWon: 0.140 - isLose: 0.000 - changed_square: 7.051 - life: 7.405 - isQuit: 0.000

Interval 81 (800000 steps performed)
10000/10000 [==============================] - 88s 9ms/step - reward: 0.2676
2760 episodes - episode_reward: 0.969 [-0.500, 1.944] - loss: 0.079 - mae: 0.367 - mean_q: 1.078 - isWon: 0.143 - isLose: 0.000 - changed_square: 7.066 - life: 7.372 - isQuit: 0.000

Interval 82 (810000 steps performed)
10000/10000 [==============================] - 86s 9ms/step - reward: 0.2646
2791 episodes - episode_reward: 0.948 [-0.500, 1.944] - loss: 0.079 - mae: 0.371 - mean_q: 1.085 - isWon: 0.142 - isLose: 0.000 - changed_square: 7.115 - life: 7.384 - isQuit: 0.000

Interval 83 (820000 steps performed)
10000/10000 [==============================] - 89s 9ms/step - reward: 0.2633
2794 episodes - episode_reward: 0.943 [-0.500, 1.944] - loss: 0.078 - mae: 0.372 - mean_q: 1.079 - isWon: 0.141 - isLose: 0.000 - changed_square: 7.077 - life: 7.383 - isQuit: 0.000

Interval 84 (830000 steps performed)
10000/10000 [==============================] - 90s 9ms/step - reward: 0.2655
2770 episodes - episode_reward: 0.958 [-0.500, 1.944] - loss: 0.078 - mae: 0.368 - mean_q: 1.082 - isWon: 0.142 - isLose: 0.000 - changed_square: 7.087 - life: 7.380 - isQuit: 0.000

Interval 85 (840000 steps performed)
10000/10000 [==============================] - 87s 9ms/step - reward: 0.2689
2792 episodes - episode_reward: 0.963 [-0.500, 1.944] - loss: 0.078 - mae: 0.361 - mean_q: 1.091 - isWon: 0.145 - isLose: 0.000 - changed_square: 7.100 - life: 7.382 - isQuit: 0.000

Interval 86 (850000 steps performed)
10000/10000 [==============================] - 86s 9ms/step - reward: 0.2655
2768 episodes - episode_reward: 0.959 [-0.500, 1.944] - loss: 0.078 - mae: 0.361 - mean_q: 1.081 - isWon: 0.142 - isLose: 0.000 - changed_square: 7.050 - life: 7.380 - isQuit: 0.000

Interval 87 (860000 steps performed)
10000/10000 [==============================] - 89s 9ms/step - reward: 0.2693
2811 episodes - episode_reward: 0.958 [-0.500, 1.944] - loss: 0.078 - mae: 0.356 - mean_q: 1.086 - isWon: 0.146 - isLose: 0.000 - changed_square: 7.065 - life: 7.385 - isQuit: 0.000

Interval 88 (870000 steps performed)
10000/10000 [==============================] - 86s 9ms/step - reward: 0.2722
2739 episodes - episode_reward: 0.994 [-0.500, 1.944] - loss: 0.077 - mae: 0.359 - mean_q: 1.096 - isWon: 0.147 - isLose: 0.000 - changed_square: 7.017 - life: 7.368 - isQuit: 0.000

Interval 89 (880000 steps performed)
10000/10000 [==============================] - 87s 9ms/step - reward: 0.2630
2812 episodes - episode_reward: 0.935 [-0.500, 1.944] - loss: 0.078 - mae: 0.364 - mean_q: 1.087 - isWon: 0.141 - isLose: 0.000 - changed_square: 7.128 - life: 7.397 - isQuit: 0.000

Interval 90 (890000 steps performed)
10000/10000 [==============================] - 85s 9ms/step - reward: 0.2725
2769 episodes - episode_reward: 0.984 [-0.500, 1.944] - loss: 0.077 - mae: 0.365 - mean_q: 1.083 - isWon: 0.147 - isLose: 0.000 - changed_square: 7.064 - life: 7.375 - isQuit: 0.000

Interval 91 (900000 steps performed)
10000/10000 [==============================] - 85s 9ms/step - reward: 0.2672
2771 episodes - episode_reward: 0.965 [-0.500, 1.944] - loss: 0.078 - mae: 0.369 - mean_q: 1.084 - isWon: 0.142 - isLose: 0.000 - changed_square: 7.129 - life: 7.372 - isQuit: 0.000

Interval 92 (910000 steps performed)
10000/10000 [==============================] - 85s 9ms/step - reward: 0.2650
2765 episodes - episode_reward: 0.958 [-0.500, 1.944] - loss: 0.077 - mae: 0.364 - mean_q: 1.084 - isWon: 0.143 - isLose: 0.000 - changed_square: 7.018 - life: 7.369 - isQuit: 0.000

Interval 93 (920000 steps performed)
10000/10000 [==============================] - 85s 8ms/step - reward: 0.2723
2765 episodes - episode_reward: 0.985 [-0.500, 1.944] - loss: 0.077 - mae: 0.365 - mean_q: 1.090 - isWon: 0.146 - isLose: 0.000 - changed_square: 7.086 - life: 7.376 - isQuit: 0.000

Interval 94 (930000 steps performed)
10000/10000 [==============================] - 88s 9ms/step - reward: 0.2693
2790 episodes - episode_reward: 0.965 [-0.500, 1.944] - loss: 0.077 - mae: 0.364 - mean_q: 1.086 - isWon: 0.146 - isLose: 0.000 - changed_square: 7.053 - life: 7.395 - isQuit: 0.000

Interval 95 (940000 steps performed)
10000/10000 [==============================] - 87s 9ms/step - reward: 0.2649
2801 episodes - episode_reward: 0.946 [-0.500, 1.944] - loss: 0.077 - mae: 0.365 - mean_q: 1.080 - isWon: 0.143 - isLose: 0.000 - changed_square: 7.084 - life: 7.398 - isQuit: 0.000

Interval 96 (950000 steps performed)
10000/10000 [==============================] - 93s 9ms/step - reward: 0.2685
2786 episodes - episode_reward: 0.964 [-0.500, 1.944] - loss: 0.079 - mae: 0.360 - mean_q: 1.076 - isWon: 0.144 - isLose: 0.000 - changed_square: 7.090 - life: 7.388 - isQuit: 0.000

Interval 97 (960000 steps performed)
10000/10000 [==============================] - 96s 10ms/step - reward: 0.2702
2768 episodes - episode_reward: 0.976 [-0.500, 1.944] - loss: 0.079 - mae: 0.362 - mean_q: 1.084 - isWon: 0.147 - isLose: 0.000 - changed_square: 6.964 - life: 7.372 - isQuit: 0.000

Interval 98 (970000 steps performed)
10000/10000 [==============================] - 91s 9ms/step - reward: 0.2742
2731 episodes - episode_reward: 1.005 [-0.500, 1.944] - loss: 0.078 - mae: 0.366 - mean_q: 1.087 - isWon: 0.145 - isLose: 0.000 - changed_square: 7.140 - life: 7.369 - isQuit: 0.000

Interval 99 (980000 steps performed)
10000/10000 [==============================] - 93s 9ms/step - reward: 0.2737
2759 episodes - episode_reward: 0.992 [-0.500, 1.944] - loss: 0.076 - mae: 0.370 - mean_q: 1.097 - isWon: 0.147 - isLose: 0.000 - changed_square: 7.078 - life: 7.370 - isQuit: 0.000

Interval 100 (990000 steps performed)
10000/10000 [==============================] - 92s 9ms/step - reward: 0.2704
done, took 9001.941 seconds

**************************************************  Test  **************************************************

Testing for 10 episodes ...
Episode 1: reward: 1.944, steps: 6
Episode 2: reward: 1.944, steps: 5
Episode 3: reward: 1.056, steps: 3
Episode 4: reward: 1.944, steps: 4
Episode 5: reward: 1.944, steps: 4
Episode 6: reward: 0.389, steps: 4
Episode 7: reward: 0.389, steps: 3
Episode 8: reward: 1.944, steps: 4
Episode 9: reward: 1.917, steps: 3
Episode 10: reward: -0.111, steps: 3

(RL) C:\Users\kator\OneDrive\ドキュメント\ResearchFloodit\gym-floodit>